
Kubernetes :::

Architecture ::::




Master Components

Below are the main components on the Kubernetes master node:

etcd cluster – a simple, distributed key value storage which is used to store the Kubernetes cluster data (such as number of pods, their state, namespace, etc), API objects and service discovery details. It is only accessible from the API server for security reasons. 

kube-apiserver – Kubernetes API server is the central management entity that receives all REST requests for modifications (to pods, services, replication sets/controllers and others), serving as frontend to the cluster. 
Also, this is the only component that communicates with the etcd cluster, making sure data is stored in etcd and agrees with the service details of the deployed pods.

kube-controller-manager – runs several distinct controller processes in the background (for example, replication controller controls number of replicas in a pod, endpoints controller populates endpoint objects like services and pods, and others) to regulate the shared state of the cluster and perform routine tasks. 

cloud-controller-manager – is responsible for managing controller processes with dependencies on the underlying cloud provider (if applicable). For example, when a controller needs to check if a node was terminated or set up routes, load balancers or volumes in the cloud infrastructure, all that is handled by the cloud-controller-manager.

kube-scheduler – helps schedule the pods (a co-located group of containers inside which our application processes are running) on the various nodes based on resource utilization. It reads the service’s operational requirements and schedules it on the best fit node. 




Node (worker) components

Below are the main components found on a (worker) node:

kubelet – the main service on a node, regularly taking in new or modified pod specifications (primarily through the kube-apiserver) and ensuring that pods and their containers are healthy and running in the desired state. This component also reports to the master on the health of the host where it is running.

kube-proxy – a proxy service that runs on each worker node to deal with individual host subnetting and expose services to the external world. It performs request forwarding to the correct pods/containers across the various isolated networks in a cluster.







	--> Kubernetes Architecture ::::
	

Kubernetes Master:

1. API Server
2. Scheduler
3. ETCD
4. Controller Manager

Worker Nodes :

1. Kubelet, 
2. Kube proxy, 
3. CRI - Container Engine



	Kubernetes Terminologies & Concepts :::

		kubectl  --> Command line Utility --> Used to interact with K8s.
		kubeadm  --> Command line Utility --> Used to Install & Configure, Attach K8s Master & Worknodes
		Pods	 --> Smallest unit of Scheduling
			 --> Pods is a collection of Container(s).

		Virtual Machine -- VMs
		Docker		-- Containers
		Kubernetes	-- Pods

Docker Assignments :::


##############
27th May. 2023
##############

		Kubernetes Terminologies & Concepts :::

		kubectl  --> Command line Utility --> Used to interact with K8s.
		kubeadm  --> Command line Utility --> Used to Install & Configure, Attach K8s Master & Worknodes
		Pods	 --> Smallest unit of Scheduling
			 --> Pods is a collection of Container(s).

		Kubernetes Master :
		  Cluster :		
			Kubernetes_Worker_Node1
			Kubernetes_Worker_Node2		
			Kubernetes_Worker_Node3
			Kubernetes_Worker_Node4


	Install & Config. Kubernetes Architecture ::

		
		Kubernetes Master :			VM
			Kubernetes_Worker_Node1		VM
			Kubernetes_Worker_Node2		VM



		
		Managed Services ::: AWS,AZ,GCP


		kubeadm 


		Create Pods 

		Write manifest files ---> *.yaml

		kubectl cli --> 

		adhoc commands 	 


		NodePort Service :::::;

			Used to expose the pods to internet:



		Pod -- nginx --> web browser --> 
		Service --> NodePort --> 30000 to 32767
		
		ClusterIP
		NodePort


		Service --> 3 -tier

			
			Front_End - UI 		Pod/Container1		--> Shd be exposed to internet. NodePort Service
			Application_Layer	Pod/Container2		--> Shd Not be exposed to internet. ClusterIP Service
			Back_End(DBase)		Pod/Container3		--> Shd Not be exposed to internet. ClusterIP Service



		NameSpace ::::

			Logical Partition of Cluster :::


		kubectl create namespace dev

[root@ip-172-31-15-73 kubernetes_Demo]# cat nginx-pod1.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod1
  namespace: dev
  labels:
    app: nginx1
    tier: dev1
spec:
  containers:
  - name: nginx-container
    image: nginx

    ports:
    - containerPort: 80		

kubectl config set-context DEV --namespace=dev
kubectl config view | grep namespace
kubectl config set-context --current --namespace=dev


Namespace ::::

DEV Team :::
 	pods 

Test Team :::

	pods	v1.0 namespace1 --> drop this.

	namespace2
		v1.1


##############
3rd Jun. 2023
##############

		Kubernetes :::
		

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod1
  namespace: dev
  labels:
    app: nginx1
    tier: dev1
spec:
  containers:
  - name: nginx-container
    image: nginx

    ports:
    - containerPort: 80	
	
	
	
pod == Created & deployed in a worknode

	High availability of application!
	
	create replicas of the pod & deploy these replicas in different worknodes.
	
	Controller Object:
	
		Deployment Controller ::
			Deploy the pod
			Replicate the pod instance
			Upgrade the the pod 
			roll back the changes
			scale up the pod 
			scale down the pod
		
		Service :
			ClusterIP	--> Internal 
			NodePort
	
	Master 
		Cluster 
			Worknode1
			Worknode2


App_team1 ==> 
created artifacts 	--> webapp.war_SNAPSHOT_V1.1 --> webapp:v1.1 --> published to Dockerhub
created artifacts 	--> webapp.war_SNAPSHOT_V1.2 --> webapp:v1.2 --> published to Dockerhub

	Deployment ::::
	
		server-target ==> webapp.war_SNAPSHOT_V1.1
		server-target ==> webapp.war_SNAPSHOT_V1.2

Kubernetes ::

			Worknode1	
			Worknode2

		pod1 	v1.1
		pod2	v1.1
		pod3 	v1.1
		
		pod1 	v1.2
		pod2	v1.2
		pod3 	v1.2
		
	Deployment controller 			==> by default use RollingUpgrade Deployment Strategy
	
	
	
	service --> loginpage.
	
		3 - tier archi.
		
			Frontend 		pod1
			appln.			pod2
			Backend 		pod3
			
	Docker publish ???? ===>
	
	End-to-End Pipeline ::::
	
	Jenkins ::
		SCM-Checkout 
		Build - App. Build 
		Docker Build 
		Publish to Dockerhub 
		Kubernetes 
			Deploy the appln. using Deployment Controller 
			
			
	Req. Infra. 
	
	VM ==> Ubuntu 
	
		Jenkin_Master						==> jdk,jenkins				8080
			Jenkins_Slave/Build_Server 		==> jdk,git,maven,docker 
			
		Kubernetes_Master					==> Kubernetes Toolsets 
			Kubernetes_Worker_Node1			==> Kubernetes Toolsets 
			Kubernetes_Worker_Node2			==> Kubernetes Toolsets 	


		Non-Prod Environments									Prod Environment
		
		Dev
		QA
		UAT														Prod 


Kubernetes (Non-Prod)Cluster 									Kubernetes (Prod)Cluster 
	Worknode1,2,3,4,5...											Worknode1,2,3,4,5...
		
NameSpace 														NameSpace 
	- Dev 															- Active_Prod 
	- QA 
	- UAT 
	
	NameSpaces ????
	
	Tools Req. 	

		open_jdk
		maven 
		git 
		docker 
		jenkins
		Kubernetes Toolsets 

		Jenkin_Master						==> jdk,jenkins
			Jenkins_Slave					==> jdk,git,maven
		QA_Server							==> jdk, tomcat
												<external_ip_qa>:8080/webapp		



##############
4th Jun. 2023
##############

	Kubernetes :::
		
	Infra. Requirements : 	
		Jenkin_Master						==> jdk,jenkins				8080
			Jenkins_Slave/Build_Server 		==> jdk,git,maven,docker 
			
		Kubernetes_Master					==> Kubernetes Toolsets 
			Kubernetes_Worker_Node1			==> Kubernetes Toolsets 
			Kubernetes_Worker_Node2			==> Kubernetes Toolsets 	
			
			
	Tools Requirements :::
			
		open_jdk
		maven 
		git 
		docker 
		jenkins
		Kubernetes Toolsets 			
	
	Connectivity/Access to Jenkins Server :::
	
		Jenkins_Master -- Slave/kubernetes
	
	
	Jenkins Pipeline :::
	
	
	scm checkout
	Appln. Build 
	Docker Image Build 
	Publish to Docker Registry
	Deploy to kubernetes cluster

pipeline {
    agent { label 'slave1' }
	

    tools {
        // Install the Maven version configured as "M3" and add it to the path.
        maven "maven"
    }

	environment {	
		DOCKERHUB_CREDENTIALS=credentials('dockerloginid')
	} 
    
    stages {
        stage('SCM Checkout') {
            steps {
                // Get some code from a GitHub repository
                git 'https://github.com/LoksaiETA/Java-mvn-app2.git'
            }
		}
        stage('Maven Build') {
            steps {
                // Run Maven on a Unix agent.
                sh "mvn -Dmaven.test.failure.ignore=true clean package"
            }
		}

        stage("Docker build"){
            steps {
				sh "docker build -t loksaieta/loksai-eta-app:${BUILD_NUMBER} ."

            }
        }
		stage('Login2DockerHub') {

			steps {
				sh 'echo $DOCKERHUB_CREDENTIALS_PSW | docker login -u $DOCKERHUB_CREDENTIALS_USR --password-stdin'
			}
		}
		stage('Push2DockerHub') {

			steps {
				sh "docker push loksaieta/loksai-eta-app:${BUILD_NUMBER}"
			}
		}
}
}






pipeline {
    agent { label 'slave1' }
	

    tools {
        // Install the Maven version configured as "M3" and add it to the path.
        maven "maven"
    }

	environment {	
		DOCKERHUB_CREDENTIALS=credentials('dockerloginid')
	} 
    
    stages {
        stage('SCM Checkout') {
            steps {
                // Get some code from a GitHub repository
                git 'https://github.com/LoksaiETA/Java-mvn-app2.git'
            }
		}
        stage('Maven Build') {
            steps {
                // Run Maven on a Unix agent.
                sh "mvn -Dmaven.test.failure.ignore=true clean package"
            }
		}

        stage("Docker build"){
            steps {
				sh "docker build -t loksaieta/loksai-eta-app:${BUILD_NUMBER} ."

            }
        }
		stage('Login2DockerHub') {

			steps {
				sh 'echo $DOCKERHUB_CREDENTIALS_PSW | docker login -u $DOCKERHUB_CREDENTIALS_USR --password-stdin'
			}
		}
		stage('Push2DockerHub') {

			steps {
				sh "docker push loksaieta/loksai-eta-app:${BUILD_NUMBER}"
			}

              post {
                success {
                  sh "echo 'Send mail on success'"
                  mail bcc: '', body: "Docker Image published to DockerHub. Please verify!", cc: 'loksaieta223@gmail.com', from: '', replyTo: '', subject: "Docker Image Publish Status", to: 'loksaieta223@gmail.com'
                }
              }
		}		

        stage('Approve - push Image to dockerhub'){
            steps{
                
                //----------------send an approval prompt-------------
                script {
                   env.APPROVED_DEPLOY = input message: 'User input required Choose "yes" | "Abort"'
                       }
                //-----------------end approval prompt------------
            }
        }

		stage('Deploy to QA Cluster') {
            steps {
			script {
				sshPublisher(publishers: [sshPublisherDesc(configName: 'SA11Kubernetes', transfers: [sshTransfer(cleanRemote: false, excludes: '', execCommand: 'kubectl apply -f k8smvndeployment.yaml', execTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes: false, patternSeparator: '[, ]+', remoteDirectory: '.', remoteDirectorySDF: false, removePrefix: '', sourceFiles: '*.yaml')], usePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)])
			}
		}
			  post {
				success {
				  sh "echo 'Send mail on success'"
				  mail bcc: '', body: "Deployment to Kubernetes Cluster is successful", cc: 'loksaieta223@gmail.com', from: '', replyTo: '', subject: "Job ${JOB_NAME} (${BUILD_NUMBER})", to: 'loksaieta223@gmail.com'
				}
				failure {
				  sh "echo 'Send mail on failure'"
				  mail bcc: '', body: "Deployment to Kubernetes Cluster Failed", cc: 'loksaieta223@gmail.com', from: '', replyTo: '', subject: "Deployment Status", to: 'loksaieta223@gmail.com'
				}
			  }	
	}
}
}


